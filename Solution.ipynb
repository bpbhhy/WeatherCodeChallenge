{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "searching-spanish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@1e4b87fe\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Starting Spark Job\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "concerned-ordering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STN---: int, WBAN: int ... 14 more fields]\n",
       "stationList: org.apache.spark.sql.DataFrame = [STN_NO: string, COUNTRY_ABBR: string]\n",
       "countryList: org.apache.spark.sql.DataFrame = [COUNTRY_ABBR: string, COUNTRY_FULL: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load files to DataFrame\n",
    "val weather = spark.read\n",
    "                .option(\"header\", true)\n",
    "                .option(\"inferSchema\", true)\n",
    "                .csv(\"data/2019\")\n",
    "                .cache()\n",
    "\n",
    "val stationList = spark.read\n",
    "                .option(\"header\", true)\n",
    "                .option(\"inferSchema\", true)\n",
    "                .csv(\"stationlist.csv\")\n",
    "\n",
    "val countryList = spark.read\n",
    "                .option(\"header\", true)\n",
    "                .option(\"inferSchema\", true)\n",
    "                .csv(\"countrylist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assigned-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STN---: integer (nullable = true)\n",
      " |-- WBAN: integer (nullable = true)\n",
      " |-- YEARMODA: integer (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: string (nullable = true)\n",
      " |-- MIN: string (nullable = true)\n",
      " |-- PRCP: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- STN_NO: string (nullable = true)\n",
      " |-- COUNTRY_ABBR: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- COUNTRY_ABBR: string (nullable = true)\n",
      " |-- COUNTRY_FULL: string (nullable = true)\n",
      "\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+----+------+\n",
      "|STN---|WBAN |YEARMODA|TEMP|DEWP|SLP   |STP   |VISIB|WDSP|MXSPD|GUST|MAX  |MIN  |PRCP |SNDP|FRSHTT|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+----+------+\n",
      "|10260 |99999|20190101|26.1|21.2|1001.9|987.5 |20.6 |9.0 |15.9 |29.7|29.8 |21.7*|0.02G|18.5|1000  |\n",
      "|10260 |99999|20190102|24.9|22.1|1020.1|1005.5|5.4  |5.6 |13.6 |22.1|27.1*|20.7 |0.48G|22.8|1000  |\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+----+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------+------------+\n",
      "|STN_NO|COUNTRY_ABBR|\n",
      "+------+------------+\n",
      "|012240|NO          |\n",
      "|020690|SW          |\n",
      "+------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------+-------------------+\n",
      "|COUNTRY_ABBR|COUNTRY_FULL       |\n",
      "+------------+-------------------+\n",
      "|AA          |ARUBA              |\n",
      "|AC          |ANTIGUA AND BARBUDA|\n",
      "+------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Show schemas and sample data\n",
    "weather.printSchema()\n",
    "stationList.printSchema()\n",
    "countryList.printSchema()\n",
    "weather.show(2, false)\n",
    "stationList.show(2, false)\n",
    "countryList.show(2, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "local-armor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationCountry: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STN_NO: string, COUNTRY_FULL: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Prepare station - country DF\n",
    "val stationCountry = stationList.join(countryList, stationList(\"COUNTRY_ABBR\") === countryList(\"COUNTRY_ABBR\"))\n",
    "            .select(\"STN_NO\", \"COUNTRY_FULL\")\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "numerous-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(YEARMODA)|max(YEARMODA)|\n",
      "+-------------+-------------+\n",
      "|     20190101|     20200101|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dateDF: Unit = ()\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get Min and Max Value of Date\n",
    "// Make sure there is no out-of-range date\n",
    "val dateDF = weather.agg(min(\"YEARMODA\"), max(\"YEARMODA\")).show\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "female-samoa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|COUNTRY_FULL|         avg_temp|\n",
      "+------------+-----------------+\n",
      "|    DJIBOUTI|90.06114457831325|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stationTempDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STN---: int, TEMP: double]\n",
       "countryTempDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STN---: int, TEMP: double ... 2 more fields]\n",
       "maxMeanTemp: Unit = ()\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* \n",
    "\n",
    "Q1: Which country had the hottest average mean temperature over the year? \n",
    "\n",
    "*/\n",
    "\n",
    "val stationTempDF = weather.select(\"STN---\", \"TEMP\")\n",
    "                            .cache()\n",
    "\n",
    "val countryTempDF = stationTempDF.join(stationCountry, stationTempDF(\"STN---\") === stationCountry(\"STN_NO\"))\n",
    "                                    .cache()\n",
    "\n",
    "val maxMeanTemp = countryTempDF.select(\"COUNTRY_FULL\", \"TEMP\")\n",
    "        .filter($\"TEMP\" < 9999.9)\n",
    "        .groupBy(\"COUNTRY_FULL\")\n",
    "        .agg(avg(\"TEMP\").as(\"avg_temp\"))\n",
    "        .orderBy(desc(\"avg_temp\"))\n",
    "        .limit(1)\n",
    "        .cache()\n",
    "        .show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "monetary-exclusive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+\n",
      "|COUNTRY_FULL|YEARMODA|rowNum|\n",
      "+------------+--------+------+\n",
      "|     ARMENIA|20190101|     1|\n",
      "|     ARMENIA|20190102|     2|\n",
      "|     ARMENIA|20190103|     3|\n",
      "|     ARMENIA|20190104|     4|\n",
      "|     ARMENIA|20190105|     5|\n",
      "|     ARMENIA|20190106|     6|\n",
      "|     ARMENIA|20190107|     7|\n",
      "|     ARMENIA|20190108|     8|\n",
      "|     ARMENIA|20190109|     9|\n",
      "|     ARMENIA|20190110|    10|\n",
      "|     ARMENIA|20190111|    11|\n",
      "|     ARMENIA|20190112|    12|\n",
      "|     ARMENIA|20190113|    13|\n",
      "|     ARMENIA|20190114|    14|\n",
      "|     ARMENIA|20190115|    15|\n",
      "|     ARMENIA|20190116|    16|\n",
      "|     ARMENIA|20190117|    17|\n",
      "|     ARMENIA|20190118|    18|\n",
      "|     ARMENIA|20190119|    19|\n",
      "|     ARMENIA|20190120|    20|\n",
      "+------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stationFRSHTTDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STN---: int, YEARMODA: int ... 1 more field]\n",
       "countryTempDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STN---: int, YEARMODA: int ... 3 more fields]\n",
       "country: Unit = ()\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* \n",
    "\n",
    "Q2: Which country had the most consecutive days of tornadoes/funnel cloud formations?\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "val stationFRSHTTDF = weather.select(\"STN---\", \"YEARMODA\", \"FRSHTT\")\n",
    "                                .cache()\n",
    "\n",
    "val countryTempDF = stationFRSHTTDF.join(stationCountry, stationTempDF(\"STN---\") === stationCountry(\"STN_NO\"))\n",
    "                                    .cache()\n",
    "\n",
    "// val country = countryTempDF.filter(($\"FRSHTT\" / 10) % 10 === 1)\n",
    "//                             .select(\"YEARMODA\", \"COUNTRY_FULL\")\n",
    "//                             .dropDuplicates()\n",
    "//                             .groupBy(\"COUNTRY_FULL\")\n",
    "//                             .agg(sort_array(collect_list(\"YEARMODA\")))\n",
    "//                             .show\n",
    "\n",
    "\n",
    "\n",
    "//                             .orderBy(\"YEARMODA\")\n",
    "//                             .select(\"YEARMODA\", \"COUNTRY_FULL\")\n",
    "//                             .show(100)\n",
    "//                             .repartition($\"COUNTRY_FULL\")\n",
    "//                             .orderBy(\"YEARMODA\")\n",
    "//                             .show   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "computational-refund",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "9: error: unclosed string literal",
     "output_type": "error",
     "traceback": [
      "<console>:9: error: unclosed string literal",
      "spark.sql(\"SELECT COUNTRY_FULL,",
      "          ^",
      "<console>:13: error: unclosed string literal",
      "            FROM table\").show",
      "                       ^",
      ""
     ]
    }
   ],
   "source": [
    "val country = countryTempDF\n",
    "        .select(\"YEARMODA\", \"COUNTRY_FULL\", \"FRSHTT\")\n",
    "        .withColumn(\"Tornado\", when(($\"FRSHTT\" / 10) % 10 === 1, 1).otherwise(0))\n",
    "        .select(\"YEARMODA\", \"COUNTRY_FULL\", \"Tornado\")\n",
    "        .dropDuplicates()\n",
    "        .createOrReplaceTempView(\"table\")\n",
    "\n",
    "spark.sql(\"SELECT COUNTRY_FULL, \n",
    "            YEARMODA, \n",
    "            Tornado, \n",
    "            GroupingSet = DATEADD(DAY, -row_number() over (partition by COUNTRY_FULL order by YEARMODA), UserDate)\n",
    "            FROM table\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "excessive-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|COUNTRY_FULL|          avg_wdsp|\n",
      "+------------+------------------+\n",
      "|     ARMENIA|457.36593182657737|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stationWDSPDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STN---: int, WDSP: double]\n",
       "countryWDSPDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STN---: int, WDSP: double ... 2 more fields]\n",
       "maxMeanTemp: Unit = ()\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* \n",
    "\n",
    "Q3: Which country had the second highest average mean wind speed over the year?\n",
    "\n",
    "*/\n",
    "\n",
    "val stationWDSPDF = weather.select(\"STN---\", \"WDSP\")\n",
    "                            .cache()\n",
    "\n",
    "val countryWDSPDF = stationWDSPDF.join(stationCountry, stationWDSPDF(\"STN---\") === stationCountry(\"STN_NO\"))\n",
    "                                    .cache()\n",
    "\n",
    "val maxMeanTemp = countryWDSPDF.select(\"COUNTRY_FULL\", \"WDSP\")\n",
    "        .filter($\"WDSP\" < 9999.9)\n",
    "        .groupBy(\"COUNTRY_FULL\")\n",
    "        .agg(avg(\"WDSP\").as(\"avg_wdsp\"))\n",
    "        .orderBy(desc(\"avg_wdsp\"))\n",
    "        .limit(2)\n",
    "        .orderBy(asc(\"avg_wdsp\"))\n",
    "        .limit(1)\n",
    "        .cache()\n",
    "        .show\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
